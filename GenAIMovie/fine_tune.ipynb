{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mistral_lora.py\n",
    "import os\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"  # or mistralai/Mistral-7B-v0.3\n",
    "TRAIN_FILE = \"dataset/train.jsonl\"\n",
    "VALID_FILE = \"dataset/valid.jsonl\"\n",
    "OUTPUT_DIR = \"mistral-lora-output\"\n",
    "BATCH_SIZE = 1    # per device; use gradient_accumulation_steps to emulate bigger batch\n",
    "EPOCHS = 3\n",
    "LR = 2e-4\n",
    "MAX_LENGTH = 1024  # token context window to use\n",
    "DEVICE_MAP = \"auto\"\n",
    "\n",
    "def load_jsonl_dataset(train_path, valid_path):\n",
    "    ds = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": valid_path})\n",
    "    return ds\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    completions = examples[\"completion\"]\n",
    "    inputs = []\n",
    "    for p, c in zip(prompts, completions):\n",
    "        # we format final training string = prompt + completion + stop token\n",
    "        txt = p + c\n",
    "        inputs.append(txt)\n",
    "    tokenized = tokenizer(inputs, truncation=True, max_length=MAX_LENGTH)\n",
    "    # labels = input_ids (causal LM)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "def main():\n",
    "    ds = load_jsonl_dataset(TRAIN_FILE, VALID_FILE)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "    # if tokenizer has no pad_token set it:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # load model in 4-bit (bitsandbytes) to save memory if supported\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        load_in_4bit=True,\n",
    "        device_map=DEVICE_MAP,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # prepare for kbit training and apply LoRA\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # tokenization + dataset mapping\n",
    "    tokenized = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=8,  # effective batch size = BATCH_SIZE * grad_accum * n_gpus\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        logging_steps=50,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    # save peft adapters only\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(\"Training done. LoRA adapters & tokenizer saved to\", OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 1️⃣ Model & tokenizer\n",
    "MODEL_NAME = \"mistral:latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# 2️⃣ LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # attention projections\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3️⃣ Load your dialogue dataset\n",
    "# Expecting a text file with one dialogue per line\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"scripts_train.txt\", \"validation\": \"scripts_val.txt\"})\n",
    "\n",
    "# Select a small subset for testing\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(2))  # Select first 2 samples from training set\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(2))  # Select first 2 samples from validation set\n",
    "\n",
    "# 4️⃣ Tokenize\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# 5️⃣ Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_dialogue_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,  # Reduce epochs for testing\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=1,  # Log every step for testing\n",
    "    save_steps=10,  # Save frequently for testing\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,  # Evaluate frequently for testing\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 6️⃣ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 7️⃣ Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# 8️⃣ Save LoRA adapter\n",
    "model.save_pretrained(\"./mistral_dialogue_lora\")\n",
    "tokenizer.save_pretrained(\"./mistral_dialogue_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune_mistral_lora.py\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 1️⃣ Model & tokenizer\n",
    "MODEL_NAME = \"mistral:latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# 2️⃣ LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # attention projections\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3️⃣ Load your dialogue dataset\n",
    "# Expecting a text file with one dialogue per line\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"scripts_train.txt\", \"validation\": \"scripts_val.txt\"})\n",
    "\n",
    "# 4️⃣ Tokenize\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# 5️⃣ Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_dialogue_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 6️⃣ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 7️⃣ Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# 8️⃣ Save LoRA adapter\n",
    "model.save_pretrained(\"./mistral_dialogue_lora\")\n",
    "tokenizer.save_pretrained(\"./mistral_dialogue_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune_mistral_lora.py\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 1️⃣ Model & tokenizer\n",
    "MODEL_NAME = \"mistral:latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# 2️⃣ LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # attention projections\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 3️⃣ Load your dialogue dataset\n",
    "# Expecting a text file with one dialogue per line\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"scripts_train.txt\", \"validation\": \"scripts_val.txt\"})\n",
    "\n",
    "# 4️⃣ Tokenize\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# 5️⃣ Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_dialogue_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 6️⃣ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 7️⃣ Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# 8️⃣ Save LoRA adapter\n",
    "model.save_pretrained(\"./mistral_dialogue_lora\")\n",
    "tokenizer.save_pretrained(\"./mistral_dialogue_lora\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolo_and_tensorflowmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
